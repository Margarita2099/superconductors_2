{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fec97ce",
   "metadata": {},
   "source": [
    "In forward selection, variables are progressively incorporated into larger and larger subsets. The\n",
    "algorithms start by training all possible single-variable machine learning models. Then, it selects\n",
    "the feature that returned the best performing classifier or regression model. In the second step, it\n",
    "creates machine learning models for all combinations of the features from the previous step with\n",
    "all remaining variables in the data. It selects the pair of features that produce the best performing\n",
    "algorithm. And it continues, adding 1 feature at a time to the feature subset from the previous step\n",
    "until a stopping criteria is met.\n",
    "\n",
    "The feature subsets are nested because they include the feature or features from the previous steps.\n",
    "By progressively evaluating promising features, forward feature selection is more efficient than\n",
    "exhaustive search.\n",
    "\n",
    "Forward feature selection has the advantage that, by starting with smaller feature subsets, it is more\n",
    "computationally efficient than other wrapper methods. However, for this same reason, it does not\n",
    "contemplate feature interactions. Or at least not until sufficient features have been added to the\n",
    "subset.\n",
    "\n",
    "Forward feature selection needs a criteria to stop the search. The most obvious stopping condition\n",
    "is when the performance of the classifier or regression model does not improve beyond a certain\n",
    "\n",
    "threshold. This has the advantage of focusing the search on performance. On the downside, the\n",
    "threshold for improvement is arbitrarily set by the user. Alternatively, we can stop the search after\n",
    "a certain number of features have been selected.\n",
    "\n",
    "In the coming paragraphs, we will implement forwardfeature selection with Scikit-learn and\n",
    "MLXtend. The 2 Python implementations are very similar. Both offer a number of features as\n",
    "stopping criteria. Scikit-learn also offers a threshold on performance improvement as a method to\n",
    "stop the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20460463",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78788423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# to select the features\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "import pandas as pd\n",
    "import csv\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from tabulate import tabulate\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b6f02",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d87197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>wtd_std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.622535</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>53.979870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.656268</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.639405</td>\n",
       "      <td>...</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.588771</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21188</th>\n",
       "      <td>106.957877</td>\n",
       "      <td>53.095769</td>\n",
       "      <td>82.515384</td>\n",
       "      <td>43.135565</td>\n",
       "      <td>1.177145</td>\n",
       "      <td>1.254119</td>\n",
       "      <td>146.88130</td>\n",
       "      <td>15.504479</td>\n",
       "      <td>65.764081</td>\n",
       "      <td>43.202659</td>\n",
       "      <td>...</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>3.223710</td>\n",
       "      <td>3.519911</td>\n",
       "      <td>1.377820</td>\n",
       "      <td>0.913658</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168889</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.496904</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21189</th>\n",
       "      <td>92.266740</td>\n",
       "      <td>49.021367</td>\n",
       "      <td>64.812662</td>\n",
       "      <td>32.867748</td>\n",
       "      <td>1.323287</td>\n",
       "      <td>1.571630</td>\n",
       "      <td>188.38390</td>\n",
       "      <td>7.353333</td>\n",
       "      <td>69.232655</td>\n",
       "      <td>50.148287</td>\n",
       "      <td>...</td>\n",
       "      <td>2.047619</td>\n",
       "      <td>2.168944</td>\n",
       "      <td>2.038991</td>\n",
       "      <td>1.594167</td>\n",
       "      <td>1.337246</td>\n",
       "      <td>1</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.212959</td>\n",
       "      <td>122.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21190</th>\n",
       "      <td>99.663190</td>\n",
       "      <td>95.609104</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>95.464320</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.530198</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>53.041104</td>\n",
       "      <td>6.756810</td>\n",
       "      <td>5.405448</td>\n",
       "      <td>...</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>4.781762</td>\n",
       "      <td>0.686962</td>\n",
       "      <td>0.450561</td>\n",
       "      <td>1</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21191</th>\n",
       "      <td>99.663190</td>\n",
       "      <td>97.095602</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>96.901083</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.640883</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>31.115202</td>\n",
       "      <td>6.756810</td>\n",
       "      <td>6.249958</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>4.665819</td>\n",
       "      <td>0.686962</td>\n",
       "      <td>0.577601</td>\n",
       "      <td>1</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462493</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21192</th>\n",
       "      <td>87.468333</td>\n",
       "      <td>86.858500</td>\n",
       "      <td>82.555758</td>\n",
       "      <td>80.458722</td>\n",
       "      <td>1.041270</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>71.75500</td>\n",
       "      <td>43.144000</td>\n",
       "      <td>29.905282</td>\n",
       "      <td>33.927941</td>\n",
       "      <td>...</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.762203</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>1.054920</td>\n",
       "      <td>0.970116</td>\n",
       "      <td>3</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21193 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_atomic_mass  wtd_mean_atomic_mass  gmean_atomic_mass  \\\n",
       "0             88.944468             57.862692          66.361592   \n",
       "1             92.729214             58.518416          73.132787   \n",
       "2             88.944468             57.885242          66.361592   \n",
       "3             88.944468             57.873967          66.361592   \n",
       "4             88.944468             57.840143          66.361592   \n",
       "...                 ...                   ...                ...   \n",
       "21188        106.957877             53.095769          82.515384   \n",
       "21189         92.266740             49.021367          64.812662   \n",
       "21190         99.663190             95.609104          99.433882   \n",
       "21191         99.663190             97.095602          99.433882   \n",
       "21192         87.468333             86.858500          82.555758   \n",
       "\n",
       "       wtd_gmean_atomic_mass  entropy_atomic_mass  wtd_entropy_atomic_mass  \\\n",
       "0                  36.116612             1.181795                 1.062396   \n",
       "1                  36.396602             1.449309                 1.057755   \n",
       "2                  36.122509             1.181795                 0.975980   \n",
       "3                  36.119560             1.181795                 1.022291   \n",
       "4                  36.110716             1.181795                 1.129224   \n",
       "...                      ...                  ...                      ...   \n",
       "21188              43.135565             1.177145                 1.254119   \n",
       "21189              32.867748             1.323287                 1.571630   \n",
       "21190              95.464320             0.690847                 0.530198   \n",
       "21191              96.901083             0.690847                 0.640883   \n",
       "21192              80.458722             1.041270                 0.895229   \n",
       "\n",
       "       range_atomic_mass  wtd_range_atomic_mass  std_atomic_mass  \\\n",
       "0              122.90607              31.794921        51.968828   \n",
       "1              122.90607              36.161939        47.094633   \n",
       "2              122.90607              35.741099        51.968828   \n",
       "3              122.90607              33.768010        51.968828   \n",
       "4              122.90607              27.848743        51.968828   \n",
       "...                  ...                    ...              ...   \n",
       "21188          146.88130              15.504479        65.764081   \n",
       "21189          188.38390               7.353333        69.232655   \n",
       "21190           13.51362              53.041104         6.756810   \n",
       "21191           13.51362              31.115202         6.756810   \n",
       "21192           71.75500              43.144000        29.905282   \n",
       "\n",
       "       wtd_std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  \\\n",
       "0                53.622535  ...          2.257143       2.213364   \n",
       "1                53.979870  ...          2.257143       1.888175   \n",
       "2                53.656268  ...          2.271429       2.213364   \n",
       "3                53.639405  ...          2.264286       2.213364   \n",
       "4                53.588771  ...          2.242857       2.213364   \n",
       "...                    ...  ...               ...            ...   \n",
       "21188            43.202659  ...          3.555556       3.223710   \n",
       "21189            50.148287  ...          2.047619       2.168944   \n",
       "21190             5.405448  ...          4.800000       4.472136   \n",
       "21191             6.249958  ...          4.690000       4.472136   \n",
       "21192            33.927941  ...          4.500000       4.762203   \n",
       "\n",
       "       wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n",
       "0               2.219783         1.368922             1.066221              1   \n",
       "1               2.210679         1.557113             1.047221              2   \n",
       "2               2.232679         1.368922             1.029175              1   \n",
       "3               2.226222         1.368922             1.048834              1   \n",
       "4               2.206963         1.368922             1.096052              1   \n",
       "...                  ...              ...                  ...            ...   \n",
       "21188           3.519911         1.377820             0.913658              1   \n",
       "21189           2.038991         1.594167             1.337246              1   \n",
       "21190           4.781762         0.686962             0.450561              1   \n",
       "21191           4.665819         0.686962             0.577601              1   \n",
       "21192           4.242641         1.054920             0.970116              3   \n",
       "\n",
       "       wtd_range_Valence  std_Valence  wtd_std_Valence  critical_temp  \n",
       "0               1.085714     0.433013         0.437059          29.00  \n",
       "1               1.128571     0.632456         0.468606          26.00  \n",
       "2               1.114286     0.433013         0.444697          19.00  \n",
       "3               1.100000     0.433013         0.440952          22.00  \n",
       "4               1.057143     0.433013         0.428809          23.00  \n",
       "...                  ...          ...              ...            ...  \n",
       "21188           2.168889     0.433013         0.496904           2.44  \n",
       "21189           0.904762     0.400000         0.212959         122.10  \n",
       "21190           3.200000     0.500000         0.400000           1.98  \n",
       "21191           2.210000     0.500000         0.462493           1.84  \n",
       "21192           1.800000     1.414214         1.500000          12.80  \n",
       "\n",
       "[21193 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../train_1.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4192c51",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e775574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14835, 80), (6358, 80))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = data.iloc[:, 0:-1]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a6c23",
   "metadata": {},
   "source": [
    "# Forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7172cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4ec268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   45.8s finished\n",
      "Features: 1/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:  1.1min finished\n",
      "Features: 2/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:  1.6min finished\n",
      "Features: 3/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:  2.0min finished\n",
      "Features: 4/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:  2.3min finished\n",
      "Features: 5/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  2.6min finished\n",
      "Features: 6/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:  2.8min finished\n",
      "Features: 7/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:  3.2min finished\n",
      "Features: 8/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  3.5min finished\n",
      "Features: 9/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:  3.6min finished\n",
      "Features: 10/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:  3.7min finished\n",
      "Features: 11/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:  4.2min finished\n",
      "Features: 12/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:  4.2min finished\n",
      "Features: 13/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:  4.3min finished\n",
      "Features: 14/15[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:  4.5min finished\n",
      "Features: 15/15"
     ]
    }
   ],
   "source": [
    "# step forward feature selection\n",
    "\n",
    "sfs = SFS(\n",
    "    estimator=RandomForestRegressor(n_estimators=5, random_state=0),\n",
    "    k_features=15,  # the number of features to retain\n",
    "    forward=True, # the direction of  the search\n",
    "    verbose=1,  # print out intermediate steps\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb40075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mean_atomic_mass',\n",
       " 'wtd_gmean_atomic_mass',\n",
       " 'range_atomic_mass',\n",
       " 'wtd_std_atomic_mass',\n",
       " 'range_fie',\n",
       " 'entropy_atomic_radius',\n",
       " 'entropy_Density',\n",
       " 'range_Density',\n",
       " 'std_ElectronAffinity',\n",
       " 'mean_ThermalConductivity',\n",
       " 'wtd_range_ThermalConductivity',\n",
       " 'mean_Valence',\n",
       " 'wtd_mean_Valence',\n",
       " 'gmean_Valence',\n",
       " 'entropy_Valence')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f94704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 76.5177175 ,  65.41137811, 122.90607   , ...,   2.4625    ,\n",
       "          2.21336384,   1.36892236],\n",
       "       [ 67.4025    ,  66.40422577,  23.115     , ...,   4.5       ,\n",
       "          4.24264069,   0.63651417],\n",
       "       [ 89.33718   ,  69.13330879, 124.90825   , ...,   2.17142857,\n",
       "          2.49146188,   1.56495725],\n",
       "       ...,\n",
       "       [ 90.30468   ,  64.31362755, 128.2426    , ...,   2.465     ,\n",
       "          2.49146188,   1.56495725],\n",
       "       [ 63.45766667,  66.73940342,  31.093     , ...,   5.24675325,\n",
       "          3.63424119,   1.01140426],\n",
       "       [ 69.17125   ,  35.42963674, 121.3276    , ...,   2.07103394,\n",
       "          2.16894354,   1.5941667 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t = sfs.transform(X_train)\n",
    "X_test_t = sfs.transform(X_test)\n",
    "\n",
    "X_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07865cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14835, 15), (6358, 15))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t.shape, X_test_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec746bf7",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8549bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_sc= min_max_scaler.fit_transform(X_train_t)\n",
    "\n",
    "X_test_sc=min_max_scaler.fit_transform(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d26aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "y_train_sc = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_sc = scaler.fit_transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20b225",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6526d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b5c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_absolute_error as mae,\n",
    "    r2_score as r2,\n",
    "    mean_absolute_percentage_error as mape,\n",
    "    mean_squared_error as mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67121d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(model, X_train_sc, X_test_sc, y_train_sc, y_test_sc, label):\n",
    "    print(f'Train MAE ({label}):', round(mae(y_train_sc, model.predict(X_train_sc)), 4))\n",
    "    print(f'Test MAE ({label}) :', round(mae(y_test_sc, model.predict(X_test_sc)), 4), '\\n')\n",
    "\n",
    "    print(f'Train R^2 ({label}):', round(r2(y_train_sc, model.predict(X_train_sc)), 4))\n",
    "    print(f'Test R^2 ({label}) :', round(r2(y_test_sc, model.predict(X_test_sc)), 4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e393e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE (LR_baseline): 0.1261\n",
      "Test MAE (LR_baseline) : 0.1087 \n",
      "\n",
      "Train R^2 (LR_baseline): 0.5744\n",
      "Test R^2 (LR_baseline) : 0.4509 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_baseline = LinearRegression().fit(X_train_sc, y_train_sc)\n",
    "report_metrics(LR_baseline, X_train_sc, X_test_sc, y_train_sc, y_test_sc, 'LR_baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83a344",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54bfab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3ff71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            fit_time  score_time  neg_mean_squared_error  \\\n",
      "RandomForestRegressor      45.892902    0.066034               -0.004685   \n",
      "XGBRegressor                7.670053    0.007209               -0.005140   \n",
      "KNeighborsRegressor         0.092373    0.048569               -0.006610   \n",
      "LGBMRegressor               0.816087    0.010529               -0.005852   \n",
      "SVR                        11.068261    0.818087               -0.012106   \n",
      "DecisionTreeRegressor       0.517493    0.002578               -0.006928   \n",
      "GradientBoostingRegressor  14.625863    0.004393               -0.009047   \n",
      "\n",
      "                           neg_mean_absolute_error  \n",
      "RandomForestRegressor                    -0.038035  \n",
      "XGBRegressor                             -0.043551  \n",
      "KNeighborsRegressor                      -0.045641  \n",
      "LGBMRegressor                            -0.049047  \n",
      "SVR                                      -0.080473  \n",
      "DecisionTreeRegressor                    -0.043362  \n",
      "GradientBoostingRegressor                -0.065592  \n"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined X_train_sc and y_train_sc\n",
    "\n",
    "list_of_models = [\n",
    "    RandomForestRegressor(),\n",
    "    XGBRegressor(),\n",
    "    KNeighborsRegressor(),\n",
    "    LGBMRegressor(),\n",
    "    SVR(),\n",
    "    DecisionTreeRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "\n",
    "list_of_model_names = [type(x).__name__ for x in list_of_models]\n",
    "cv_results = pd.DataFrame(\n",
    "    data=0.0,\n",
    "    index=list_of_model_names,\n",
    "    columns=['fit_time', 'score_time', 'neg_mean_squared_error', 'neg_mean_absolute_error'])\n",
    "\n",
    "for model in list_of_models:\n",
    "    cv_result = cross_validate(\n",
    "        estimator=model,\n",
    "        X=X_train_sc,\n",
    "        y=y_train_sc,\n",
    "        scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "        cv=30,\n",
    "        n_jobs=-1)\n",
    "\n",
    "    cv_results.loc[type(model).__name__] = [\n",
    "        np.mean(cv_result['fit_time']),\n",
    "        np.mean(cv_result['score_time']),\n",
    "        np.mean(cv_result['test_neg_mean_squared_error']),\n",
    "        np.mean(cv_result['test_neg_mean_absolute_error'])  # Removed the negative sign here\n",
    "    ]\n",
    "\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31be9a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margarita\\AppData\\Local\\Temp\\ipykernel_24488\\377911579.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_sc, y_train_sc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.010474788203692785\n",
      "Mean Absolute Error (MAE): 0.0670117528082491\n",
      "R-squared (R2): 0.6964966640866743\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_sc, y_train_sc)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_sc)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test_sc, y_pred)\n",
    "mae = mean_absolute_error(y_test_sc, y_pred)\n",
    "r2 = r2_score(y_test_sc, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d4d9141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.009969758316757437\n",
      "Mean Absolute Error (MAE): 0.06784181844209433\n",
      "R-squared (R2): 0.7111297289697209\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_sc, y_train_sc)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_sc)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test_sc, y_pred)\n",
    "mae = mean_absolute_error(y_test_sc, y_pred)\n",
    "r2 = r2_score(y_test_sc, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514c13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
